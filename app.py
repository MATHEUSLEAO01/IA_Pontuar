import streamlit as st
import pandas as pd
from openai import OpenAI
import pdfplumber
from PIL import Image
import pytesseract
import camelot
from PyPDF2 import PdfReader
import io
from transformers import pipeline

# -----------------------------
# Inicializa√ß√£o
# -----------------------------
st.set_page_config(page_title="IA Leitora de Planilhas Avan√ßada", layout="wide")
st.title("üìä IA Leitora de Planilhas Avan√ßada - Pontuar Tech")
st.markdown("1Ô∏è‚É£ Envie planilha, PDF ou imagem ‚Üí 2Ô∏è‚É£ Informe o tipo ‚Üí 3Ô∏è‚É£ Fa√ßa uma pergunta ‚Üí 4Ô∏è‚É£ Veja a resposta!")

# OpenAI
client = OpenAI(api_key=st.secrets["general"]["OPENAI_API_KEY"])

# Hugging Face como fallback gratuito
hf_pipeline = pipeline("text-generation", model="bigscience/bloom-560m", device=-1)  # CPU

# -----------------------------
# Sess√£o
# -----------------------------
if "historico" not in st.session_state:
    st.session_state["historico"] = []

# -----------------------------
# Upload
# -----------------------------
uploaded_file = st.file_uploader(
    "üìÇ Envie planilha (.xlsx), PDF ou imagem (.png, .jpg, .jpeg)",
    type=["xlsx", "pdf", "png", "jpg", "jpeg"]
)

# -----------------------------
# Fun√ß√µes auxiliares
# -----------------------------
def extrair_texto_pdf(file):
    texto = ""
    try:
        with pdfplumber.open(file) as pdf:
            for page in pdf.pages:
                texto += page.extract_text() or ""
    except:
        pass
    return texto

def extrair_tabelas_pdf(file):
    try:
        tables = camelot.read_pdf(file, pages="all", flavor="stream")
        if tables and len(tables) > 0:
            df = tables[0].df
            df.columns = df.iloc[0]
            df = df[1:]
            return df
    except:
        return None
    return None

def extrair_texto_imagem(file):
    imagem = Image.open(file)
    return pytesseract.image_to_string(imagem, lang="por")

# -----------------------------
# Processamento de arquivo
# -----------------------------
texto_extraido = None
df = None

if uploaded_file:
    nome = uploaded_file.name.lower()
    try:
        if nome.endswith(".xlsx"):
            df = pd.read_excel(uploaded_file)
            st.success("‚úÖ Planilha Excel carregada!")
        elif nome.endswith(".pdf"):
            df = extrair_tabelas_pdf(uploaded_file)
            if df is not None:
                st.success("‚úÖ Tabela detectada e carregada do PDF!")
            else:
                texto_extraido = extrair_texto_pdf(uploaded_file)
                st.info("üìÑ Nenhuma tabela detectada ‚Äî texto extra√≠do para an√°lise.")
        elif nome.endswith((".png", ".jpg", ".jpeg")):
            texto_extraido = extrair_texto_imagem(uploaded_file)
            st.success("‚úÖ Texto extra√≠do da imagem!")
    except Exception as e:
        st.error(f"‚ùå Erro ao processar: {e}")
        st.stop()

# -----------------------------
# Tipo de conte√∫do
# -----------------------------
tipo_planilha = st.text_input("üóÇ Qual o tipo de conte√∫do? (ex.: vendas, gastos, notas fiscais...)")

# -----------------------------
# Caixa de pergunta
# -----------------------------
pergunta = st.text_input("üí¨ Sua pergunta:")

# -----------------------------
# Fun√ß√£o para gerar resposta HF
# -----------------------------
def gerar_resposta_hf(conteudo):
    try:
        if len(conteudo) > 1000:  # limitar para n√£o travar o modelo
            conteudo = conteudo[-1000:]
        saida = hf_pipeline(f"Responda detalhadamente: {conteudo}", max_new_tokens=256)
        return saida[0]["generated_text"]
    except:
        return "N√£o foi poss√≠vel gerar resposta gratuita."

# -----------------------------
# Processamento
# -----------------------------
if st.button("üîç Perguntar") and (df is not None or texto_extraido) and tipo_planilha:
    try:
        if df is not None:
            resumo = {
                "tipo_planilha": tipo_planilha,
                "colunas": list(df.columns),
                "amostra": df.head(10).to_dict(orient="records"),
            }
            conteudo = f"Resumo da planilha:\n{resumo}\nPergunta: {pergunta}"
        else:
            conteudo = f"Texto detectado:\n{texto_extraido}\nPergunta: {pergunta}"

        prompt_system = (
            "Voc√™ √© um assistente especialista em an√°lise de planilhas, PDFs e textos financeiros em portugu√™s. "
            "Responda detalhadamente. Se n√£o houver informa√ß√£o, diga 'N√£o encontrado'."
        )

        # Tenta OpenAI GPT-4o-mini
        try:
            resposta = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": prompt_system},
                    {"role": "user", "content": conteudo}
                ]
            )
            texto_completo = resposta.choices[0].message.content.strip()
        except Exception:
            # Se falhar, usa Hugging Face
            texto_completo = gerar_resposta_hf(conteudo)

        st.subheader("‚úÖ Resposta Detalhada:")
        st.write(texto_completo)
        st.session_state["historico"].append({"pergunta": pergunta, "resposta": texto_completo})

    except Exception as e:
        st.error(f"Erro: {e}")

# -----------------------------
# Hist√≥rico
# -----------------------------
if st.session_state["historico"]:
    st.subheader("üìú Hist√≥rico de perguntas recentes")
    for h in reversed(st.session_state["historico"][-5:]):
        st.markdown(f"**Pergunta:** {h['pergunta']}  \n**Resposta:** {h['resposta']}")
