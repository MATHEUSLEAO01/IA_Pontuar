import streamlit as st
import pandas as pd
from openai import OpenAI
import pdfplumber
import camelot
from PyPDF2 import PdfReader
from PIL import Image
import io
import requests

# -----------------------------
# Inicializa√ß√£o
# -----------------------------
st.set_page_config(page_title="IA Leitora Avan√ßada", layout="wide")
st.title("üìä IA Leitora de Planilhas Avan√ßada - Pontuar Tech")
st.markdown("1Ô∏è‚É£ Envie planilha, PDF ou imagem ‚Üí 2Ô∏è‚É£ Informe o tipo ‚Üí 3Ô∏è‚É£ Fa√ßa uma pergunta ‚Üí 4Ô∏è‚É£ Veja a resposta!")

# OpenAI
client = OpenAI(api_key=st.secrets["general"]["OPENAI_API_KEY"])

# Sess√£o
if "historico" not in st.session_state:
    st.session_state["historico"] = []

# -----------------------------
# Upload de arquivo
# -----------------------------
uploaded_file = st.file_uploader("üìÇ Envie planilha (.xlsx), PDF ou imagem (.png, .jpg, .jpeg)", type=["xlsx","pdf","png","jpg","jpeg"])

# Fun√ß√µes auxiliares
def extrair_tabelas_pdf(file):
    try:
        tables = camelot.read_pdf(file, pages="all", flavor="stream")
        if tables and len(tables) > 0:
            df = tables[0].df
            df.columns = df.iloc[0]
            df = df[1:]
            return df
    except Exception:
        pass
    return None

def extrair_texto_pdf(file):
    texto = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            texto += page.extract_text() or ""
    return texto

def extrair_texto_imagem(file):
    """Usa OpenAI para OCR (gratuito)"""
    # Converter imagem para bytes
    imagem = Image.open(file)
    buf = io.BytesIO()
    imagem.save(buf, format="PNG")
    buf.seek(0)
    
    # Fallback gratuito via OCR.Space API
    # OBS: pode usar qualquer servi√ßo OCR gratuito se quiser
    payload = {"apikey": "helloworld", "language": "por"}
    files = {"file": buf}
    try:
        r = requests.post("https://api.ocr.space/parse/image", data=payload, files=files)
        result = r.json()
        texto = result.get("ParsedResults")[0]["ParsedText"]
        return texto
    except:
        return "N√£o foi poss√≠vel extrair texto da imagem."

# -----------------------------
# Processamento
# -----------------------------
df = None
texto_extraido = None

if uploaded_file:
    nome = uploaded_file.name.lower()
    try:
        if nome.endswith(".xlsx"):
            df = pd.read_excel(uploaded_file)
            st.success("‚úÖ Planilha Excel carregada!")
        elif nome.endswith(".pdf"):
            df = extrair_tabelas_pdf(uploaded_file)
            if df is not None:
                st.success("‚úÖ Tabela detectada e carregada do PDF!")
            else:
                texto_extraido = extrair_texto_pdf(uploaded_file)
                st.info("üìÑ Nenhuma tabela detectada ‚Äî texto extra√≠do para an√°lise.")
        elif nome.endswith((".png","jpg","jpeg")):
            texto_extraido = extrair_texto_imagem(uploaded_file)
            st.success("‚úÖ Texto extra√≠do da imagem!")
    except Exception as e:
        st.error(f"‚ùå Erro ao processar: {e}")
        st.stop()

# Tipo de conte√∫do
tipo_planilha = st.text_input("üóÇ Qual o tipo de conte√∫do? (ex.: vendas, gastos, notas fiscais...)")

# Pergunta
pergunta = st.text_input("üí¨ Sua pergunta:")

# -----------------------------
# Processamento da pergunta
# -----------------------------
def gerar_resposta_openai(conteudo):
    prompt = (
        "Voc√™ √© um assistente especialista em an√°lise de planilhas, PDFs e textos financeiros em portugu√™s. "
        "Responda detalhadamente. Se n√£o encontrar a informa√ß√£o, diga 'N√£o encontrado'."
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": conteudo}
            ]
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return None

def gerar_resposta_gratuita(conteudo):
    # Fallback gratuito usando API HuggingFace GPT2 simples
    try:
        r = requests.post(
            "https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct",
            headers={"Authorization": "Bearer hf_demo"},
            json={"inputs": conteudo}
        )
        result = r.json()
        if isinstance(result, list):
            return result[0]["generated_text"]
        elif "error" in result:
            return "N√£o foi poss√≠vel gerar resposta gratuita."
    except:
        return "N√£o foi poss√≠vel gerar resposta gratuita."
    return "N√£o encontrado."

if st.button("üîç Perguntar") and (df is not None or texto_extraido) and tipo_planilha and pergunta:
    if df is not None:
        resumo = {
            "tipo_planilha": tipo_planilha,
            "colunas": list(df.columns),
            "amostra": df.head(10).to_dict(orient="records"),
        }
        conteudo = f"Resumo da planilha:\n{resumo}\nPergunta: {pergunta}"
    else:
        conteudo = f"Texto detectado:\n{texto_extraido}\nPergunta: {pergunta}"
    
    resposta = gerar_resposta_openai(conteudo)
    if not resposta:
        resposta = gerar_resposta_gratuita(conteudo)
    
    st.subheader("‚úÖ Resposta Detalhada:")
    st.write(resposta)
    st.session_state["historico"].append({"pergunta": pergunta, "resposta": resposta})

# -----------------------------
# Hist√≥rico
# -----------------------------
if st.session_state["historico"]:
    st.subheader("üìú Hist√≥rico de perguntas recentes")
    for h in reversed(st.session_state["historico"][-5:]):
        st.markdown(f"**Pergunta:** {h['pergunta']}  \n**Resposta:** {h['resposta']}")
